{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5847d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "import time\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333eaa0",
   "metadata": {},
   "source": [
    "# #1 - Write model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c09ff3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input tensor to a vector of size 784\n",
    "        x = torch.relu(self.fc1(x))  # Apply first layer with ReLU activation\n",
    "        x = self.fc2(x)  # Apply second layer (outputs logits for 10 classes)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    # Download and prepare the MNIST dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = datasets.MNIST(root='/opt/ml/input/data/train', train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    model = SimpleNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1} complete')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), '/opt/ml/model/model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc9148",
   "metadata": {},
   "source": [
    "# #2 - Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c426d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.8\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install torch torchvision\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the training script into the container\n",
    "COPY train.py .\n",
    "\n",
    "# Command to run when the container starts\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e767fab",
   "metadata": {},
   "source": [
    "# #3 - Create and Register Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e034db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\r\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\r\n",
      "Configure a credential helper to remove this warning. See\r\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\r\n",
      "\r\n",
      "Login Succeeded\r\n"
     ]
    }
   ],
   "source": [
    "# Enable ECR Access\n",
    "!$(aws ecr get-login --no-include-email --region us-east-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb228a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'my-custom-algorithm' already exists in the registry with id '038462750455'\r\n"
     ]
    }
   ],
   "source": [
    "# Create ECR Repository\n",
    "!aws ecr create-repository --repository-name my-custom-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ec8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "63 packages excluded due to repository priority protections\n",
      "No packages marked for update\n",
      "Installing docker\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "Cleaning repos: amzn2-core amzn2extra-docker amzn2extra-kernel-5.10\n",
      "              : amzn2extra-livepatch amzn2extra-lustre amzn2extra-python3.8\n",
      "              : centos-extras\n",
      "              : copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxidmap\n",
      "              : docker-ce-stable nvidia-container-toolkit\n",
      "34 metadata files removed\n",
      "17 sqlite files removed\n",
      "0 metadata files removed\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-lustre                                        | 2.5 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "nvidia-container-toolkit/x86_64/signature                |  833 B     00:00     \n",
      "nvidia-container-toolkit/x86_64/signature                | 2.1 kB     00:03 !!! \n",
      "(1/15): amzn2-core/2/x86_64/group_gz                       | 2.7 kB   00:00     \n",
      "(2/15): amzn2-core/2/x86_64/updateinfo                     | 983 kB   00:00     \n",
      "(3/15): amzn2extra-docker/2/x86_64/updateinfo              |  20 kB   00:00     \n",
      "(4/15): amzn2extra-livepatch/2/x86_64/updateinfo           |  21 kB   00:00     \n",
      "(5/15): amzn2extra-livepatch/2/x86_64/primary_db           |  60 kB   00:00     \n",
      "(6/15): amzn2extra-lustre/2/x86_64/primary_db              |  10 kB   00:00     \n",
      "(7/15): amzn2extra-python3.8/2/x86_64/updateinfo           | 6.1 kB   00:00     \n",
      "(8/15): amzn2extra-docker/2/x86_64/primary_db              | 114 kB   00:00     \n",
      "(9/15): amzn2extra-python3.8/2/x86_64/primary_db           |  61 kB   00:00     \n",
      "(10/15): amzn2extra-kernel-5.10/2/x86_64/updateinfo        |  91 kB   00:00     \n",
      "(11/15): amzn2extra-kernel-5.10/2/x86_64/primary_db        |  31 MB   00:00     \n",
      "(12/15): centos-extras/primary_db                          | 253 kB   00:00     \n",
      "(13/15): nvidia-container-toolkit/x86_64/primary           |  13 kB   00:00     \n",
      "(14/15): copr:copr.fedorainfracloud.org:vbatts:shadow-util | 6.1 kB   00:00     \n",
      "(15/15): amzn2-core/2/x86_64/primary_db                    |  71 MB   00:01     \n",
      "nvidia-container-toolkit                                                  83/83\n",
      "63 packages excluded due to repository priority protections\n",
      "Package docker-25.0.6-1.amzn2.0.2.x86_64 already installed and latest version\n",
      "Nothing to do\n",
      "  2  httpd_modules            available  \u001b[0m  [ =1.0  =stable ]\n",
      "  3  memcached1.5             available  \u001b[0m  \\\n",
      "        [ =1.5.1  =1.5.16  =1.5.17 ]\n",
      "  9  R3.4                     available  \u001b[0m  [ =3.4.3  =stable ]\n",
      " 10  rust1                    available  \u001b[0m  \\\n",
      "        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0\n",
      "          =stable ]\n",
      " 18  libreoffice              available  \u001b[0m  \\\n",
      "        [ =5.0.6.2_15  =5.3.6.1  =stable ]\n",
      " 19  gimp                     available  \u001b[0m  [ =2.8.22 ]\n",
      " 20 †\u001b[94mdocker=latest            enabled    \u001b[0m  \\\n",
      "        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]\n",
      " 21  mate-desktop1.x          available  \u001b[0m  \\\n",
      "        [ =1.19.0  =1.20.0  =stable ]\n",
      " 22  GraphicsMagick1.3        available  \u001b[0m  \\\n",
      "        [ =1.3.29  =1.3.32  =1.3.34  =stable ]\n",
      " 24  epel                     available  \u001b[0m  [ =7.11  =stable ]\n",
      " 25  testing                  available  \u001b[0m  [ =1.0  =stable ]\n",
      " 26  ecs                      available  \u001b[0m  [ =stable ]\n",
      " 27 †corretto8                available  \u001b[0m  \\\n",
      "        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232\n",
      "          =1.8.0_242  =stable ]\n",
      " 32  lustre2.10               available  \u001b[0m  \\\n",
      "        [ =2.10.5  =2.10.8  =stable ]\n",
      " 34  lynis                    available  \u001b[0m  [ =stable ]\n",
      " 36  BCC                      available  \u001b[0m  [ =0.x  =stable ]\n",
      " 37  mono                     available  \u001b[0m  [ =5.x  =stable ]\n",
      " 38  nginx1                   available  \u001b[0m  [ =stable ]\n",
      " 40  mock                     available  \u001b[0m  [ =stable ]\n",
      " 43  \u001b[94mlivepatch=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 44 \u001b[93m*\u001b[0m\u001b[94mpython3.8=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 45  haproxy2                 available  \u001b[0m  [ =stable ]\n",
      " 46  collectd                 available  \u001b[0m  [ =stable ]\n",
      " 47  aws-nitro-enclaves-cli   available  \u001b[0m  [ =stable ]\n",
      " 48  R4                       available  \u001b[0m  [ =stable ]\n",
      "  _  kernel-5.4               available  \u001b[0m  [ =stable ]\n",
      " 50  selinux-ng               available  \u001b[0m  [ =stable ]\n",
      " 52  tomcat9                  available  \u001b[0m  [ =stable ]\n",
      " 53  unbound1.13              available  \u001b[0m  [ =stable ]\n",
      " 54 †mariadb10.5              available  \u001b[0m  [ =stable ]\n",
      " 55  \u001b[94mkernel-5.10=latest       enabled    \u001b[0m  [ =stable ]\n",
      " 56  redis6                   available  \u001b[0m  [ =stable ]\n",
      " 58 †postgresql12             available  \u001b[0m  [ =stable ]\n",
      " 59 †postgresql13             available  \u001b[0m  [ =stable ]\n",
      " 60  mock2                    available  \u001b[0m  [ =stable ]\n",
      " 61  dnsmasq2.85              available  \u001b[0m  [ =stable ]\n",
      " 62  kernel-5.15              available  \u001b[0m  [ =stable ]\n",
      " 63 †postgresql14             available  \u001b[0m  [ =stable ]\n",
      " 64  firefox                  available  \u001b[0m  [ =stable ]\n",
      " 65  \u001b[94mlustre=latest            enabled    \u001b[0m  [ =stable ]\n",
      " 66 †php8.1                   available  \u001b[0m  [ =stable ]\n",
      " 67  awscli1                  available  \u001b[0m  [ =stable ]\n",
      " 68 †php8.2                   available  \u001b[0m  [ =stable ]\n",
      " 69  dnsmasq                  available  \u001b[0m  [ =stable ]\n",
      " 70  unbound1.17              available  \u001b[0m  [ =stable ]\n",
      " 72  collectd-python3         available  \u001b[0m  [ =stable ]\n",
      "\u001b[93m*\u001b[0m Extra topic has reached end of support.\n",
      "† Note on end-of-support. Use 'info' subcommand.\n",
      "Redirecting to /bin/systemctl start docker.service\n"
     ]
    }
   ],
   "source": [
    "# Install Docker\n",
    "!sudo yum update -y\n",
    "!sudo amazon-linux-extras install docker -y\n",
    "!sudo service docker start\n",
    "!sudo usermod -a -G docker ec2-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac82e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 296B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 296B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              0.2s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (9/9) FINISHED                                 docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 296B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              0.2s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/library/python:3.8@sha256:d411270700143fa2683cc8  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.58kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] RUN pip install torch torchvision                         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] COPY train.py .                                           0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:9482c461afc5f7e3d76c5f58bdc43e0180d50c49e41d4  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/my-custom-algorithm                     0.0s\n",
      "\u001b[0m\u001b[?25hThe push refers to repository [038462750455.dkr.ecr.us-east-2.amazonaws.com/my-custom-algorithm]\n",
      "\n",
      "\u001b[1B1cc5bca5: Preparing \n",
      "\u001b[1B5723c158: Preparing \n",
      "\u001b[1B4777df1b: Preparing \n",
      "\u001b[1B710ca3c7: Preparing \n",
      "\u001b[1Be4d52b5a: Preparing \n",
      "\u001b[1B8afd69b3: Preparing \n",
      "\u001b[1B433c3a29: Preparing \n",
      "\u001b[1Bc7a486d9: Preparing \n",
      "\u001b[1Ba6961052: Preparing \n",
      "\u001b[2Ba6961052: Layer already exists \u001b[5A\u001b[2Klatest: digest: sha256:518b5da80197eed526f3a32c1f919d8cdba5412f5f2758405aff9d64eb5f0384 size: 2424\n"
     ]
    }
   ],
   "source": [
    "# Get the AWS account ID and region\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Define the repository name and ECR URI\n",
    "repository_name = \"my-custom-algorithm\"\n",
    "ecr_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}\"\n",
    "\n",
    "# Docker commands to build, tag, and push the Docker image\n",
    "!docker build -t my-custom-algorithm .\n",
    "!docker tag my-custom-algorithm:latest {ecr_uri}:latest\n",
    "!docker push {ecr_uri}:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbcf204",
   "metadata": {},
   "source": [
    "# #4 - Train Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75000d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: simpleNN-2024-10-22-02-51-32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22 02:51:35 Starting - Starting the training job...\n",
      "2024-10-22 02:51:48 Starting - Preparing the instances for training...\n",
      "2024-10-22 02:52:33 Downloading - Downloading the training image..................\n",
      "2024-10-22 02:55:15 Training - Training image download completed. Training in progress.\u001b[34m#0150.3%#0150.7%#0151.0%#0151.3%#0151.7%#0152.0%#0152.3%#0152.6%#0153.0%#0153.3%#0153.6%#0154.0%#0154.3%#0154.6%#0155.0%#0155.3%#0155.6%#0156.0%#0156.3%#0156.6%#0156.9%#0157.3%#0157.6%#0157.9%#0158.3%#0158.6%#0158.9%#0159.3%#0159.6%#0159.9%#01510.2%#01510.6%#01510.9%#01511.2%#01511.6%#01511.9%#01512.2%#01512.6%#01512.9%#01513.2%#01513.6%#01513.9%#01514.2%#01514.5%#01514.9%#01515.2%#01515.5%#01515.9%#01516.2%#01516.5%#01516.9%#01517.2%#01517.5%#01517.9%#01518.2%#01518.5%#01518.8%#01519.2%#01519.5%#01519.8%#01520.2%#01520.5%#01520.8%#01521.2%#01521.5%#01521.8%#01522.1%#01522.5%#01522.8%#01523.1%#01523.5%#01523.8%#01524.1%#01524.5%#01524.8%#01525.1%#01525.5%#01525.8%#01526.1%#01526.4%#01526.8%#01527.1%#01527.4%#01527.8%#01528.1%#01528.4%#01528.8%#01529.1%#01529.4%#01529.8%#01530.1%#01530.4%#01530.7%#01531.1%#01531.4%#01531.7%#01532.1%#01532.4%#01532.7%#01533.1%#01533.4%#01533.7%#01534.0%#01534.4%#01534.7%#01535.0%#01535.4%#01535.7%#01536.0%#01536.4%#01536.7%#01537.0%#01537.4%#01537.7%#01538.0%#01538.3%#01538.7%#01539.0%#01539.3%#01539.7%#01540.0%#01540.3%#01540.7%#01541.0%#01541.3%#01541.7%#01542.0%#01542.3%#01542.6%#01543.0%#01543.3%#01543.6%#01544.0%#01544.3%#01544.6%#01545.0%#01545.3%#01545.6%#01545.9%#01546.3%#01546.6%#01546.9%#01547.3%#01547.6%#01547.9%#01548.3%#01548.6%#01548.9%#01549.3%#01549.6%#01549.9%#01550.2%#01550.6%#01550.9%#01551.2%#01551.6%#01551.9%#01552.2%#01552.6%#01552.9%#01553.2%#01553.6%#01553.9%#01554.2%#01554.5%#01554.9%#01555.2%#01555.5%#01555.9%#01556.2%#01556.5%#01556.9%#01557.2%#01557.5%#01557.9%#01558.2%#01558.5%#01558.8%#01559.2%#01559.5%#01559.8%#01560.2%#01560.5%#01560.8%#01561.2%#01561.5%#01561.8%#01562.1%#01562.5%#01562.8%#01563.1%#01563.5%#01563.8%#01564.1%#01564.5%#01564.8%#01565.1%#01565.5%#01565.8%#01566.1%#01566.4%#01566.8%#01567.1%#01567.4%#01567.8%#01568.1%#01568.4%#01568.8%#01569.1%#01569.4%#01569.8%#01570.1%#01570.4%#01570.7%#01571.1%#01571.4%#01571.7%#01572.1%#01572.4%#01572.7%#01573.1%#01573.4%#01573.7%#01574.0%#01574.4%#01574.7%#01575.0%#01575.4%#01575.7%#01576.0%#01576.4%#01576.7%#01577.0%#01577.4%#01577.7%#01578.0%#01578.3%#01578.7%#01579.0%#01579.3%#01579.7%#01580.0%#01580.3%#01580.7%#01581.0%#01581.3%#01581.7%#01582.0%#01582.3%#01582.6%#01583.0%#01583.3%#01583.6%#01584.0%#01584.3%#01584.6%#01585.0%#01585.3%#01585.6%#01585.9%#01586.3%#01586.6%#01586.9%#01587.3%#01587.6%#01587.9%#01588.3%#01588.6%#01588.9%#01589.3%#01589.6%#01589.9%#01590.2%#01590.6%#01590.9%#01591.2%#01591.6%#01591.9%#01592.2%#01592.6%#01592.9%#01593.2%#01593.6%#01593.9%#01594.2%#01594.5%#01594.9%#01595.2%#01595.5%#01595.9%#01596.2%#01596.5%#01596.9%#01597.2%#01597.5%#01597.9%#01598.2%#01598.5%#01598.8%#01599.2%#01599.5%#01599.8%#015100.0%\u001b[0m\n",
      "\u001b[34m#015100.0%\u001b[0m\n",
      "\u001b[34m#0152.0%#0154.0%#0156.0%#0157.9%#0159.9%#01511.9%#01513.9%#01515.9%#01517.9%#01519.9%#01521.9%#01523.8%#01525.8%#01527.8%#01529.8%#01531.8%#01533.8%#01535.8%#01537.8%#01539.7%#01541.7%#01543.7%#01545.7%#01547.7%#01549.7%#01551.7%#01553.7%#01555.6%#01557.6%#01559.6%#01561.6%#01563.6%#01565.6%#01567.6%#01569.6%#01571.5%#01573.5%#01575.5%#01577.5%#01579.5%#01581.5%#01583.5%#01585.5%#01587.4%#01589.4%#01591.4%#01593.4%#01595.4%#01597.4%#01599.4%#015100.0%\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mFailed to download (trying next):\u001b[0m\n",
      "\u001b[34m<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1149)>\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /opt/ml/input/data/train/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/train/MNIST/raw/train-images-idx3-ubyte.gz to /opt/ml/input/data/train/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mFailed to download (trying next):\u001b[0m\n",
      "\u001b[34m<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1149)>\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /opt/ml/input/data/train/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/train/MNIST/raw/train-labels-idx1-ubyte.gz to /opt/ml/input/data/train/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mFailed to download (trying next):\u001b[0m\n",
      "\u001b[34m<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1149)>\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /opt/ml/input/data/train/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/train/MNIST/raw/t10k-images-idx3-ubyte.gz to /opt/ml/input/data/train/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mFailed to download (trying next):\u001b[0m\n",
      "\u001b[34m<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1149)>\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /opt/ml/input/data/train/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mExtracting /opt/ml/input/data/train/MNIST/raw/t10k-labels-idx1-ubyte.gz to /opt/ml/input/data/train/MNIST/raw\u001b[0m\n",
      "\u001b[34mEpoch 1 complete\u001b[0m\n",
      "\u001b[34mEpoch 2 complete\u001b[0m\n",
      "\u001b[34mEpoch 3 complete\u001b[0m\n",
      "\u001b[34mEpoch 4 complete\u001b[0m\n",
      "\u001b[34mEpoch 5 complete\u001b[0m\n",
      "\u001b[34mEpoch 6 complete\u001b[0m\n",
      "\u001b[34mEpoch 7 complete\u001b[0m\n",
      "\u001b[34mEpoch 8 complete\u001b[0m\n",
      "\u001b[34mEpoch 9 complete\u001b[0m\n",
      "\u001b[34mEpoch 10 complete\u001b[0m\n",
      "\u001b[34m#015100.0%\u001b[0m\n",
      "\n",
      "2024-10-22 02:57:05 Uploading - Uploading generated training model\n",
      "2024-10-22 02:57:05 Completed - Training job completed\n",
      "Training seconds: 291\n",
      "Billable seconds: 291\n",
      "Model saved at: s3://training-models-on-amazon-sagemaker/mnist/simpleNN/simpleNN-2024-10-22-02-51-32/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Specify variables\n",
    "bucket_name = \"training-models-on-amazon-sagemaker\"\n",
    "prefix = 'mnist/simpleNN'\n",
    "bucket_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "\n",
    "# Get AWS account and region dynamically\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# ECR repository details (the updated custom container)\n",
    "repository_name = \"my-custom-algorithm\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define the custom Estimator using the updated ECR image\n",
    "custom_estimator = Estimator(\n",
    "    image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}:latest\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=bucket_path,  # Output path for the trained model\n",
    ")\n",
    "\n",
    "# Start the training job\n",
    "training_job_name = f'simpleNN-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "custom_estimator.fit(job_name=training_job_name)\n",
    "\n",
    "# Fetch training job details\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Retrieve the training job description\n",
    "training_info = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "# Print the S3 location where the model is stored\n",
    "model_s3_uri = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Model saved at: {model_s3_uri}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
